{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baekhyunjung/study_nlp/blob/main/continual%20learning/NERDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqEOOBMKVHXp",
        "outputId": "1a677406-bcb8-431f-9910-3800c070b7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting NERDA-Con\n",
            "  Downloading NERDA_Con-0.0-py3-none-any.whl (21 kB)\n",
            "Collecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from NERDA-Con) (1.3.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from NERDA-Con) (3.7)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 6.6 MB/s \n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from NERDA-Con) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA-Con) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA-Con) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA-Con) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA-Con) (2022.6.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->NERDA-Con) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->NERDA-Con) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->NERDA-Con) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->NERDA-Con) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->NERDA-Con) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->NERDA-Con) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->NERDA-Con) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->NERDA-Con) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA-Con) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA-Con) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA-Con) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 36.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA-Con) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA-Con) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->NERDA-Con) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->NERDA-Con) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA-Con) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA-Con) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA-Con) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA-Con) (3.0.4)\n",
            "Building wheels for collected packages: progressbar, sklearn\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=c2a148ee280ced3ca2642817a119fd725bfc074f1c28f89176b58afaf79f6353\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=93eb798346b831f49d374f59136919639d99263c42e16f913297c1c89ea3c230\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built progressbar sklearn\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sklearn, pyconll, progressbar, NERDA-Con\n",
            "Successfully installed NERDA-Con-0.0 huggingface-hub-0.9.1 progressbar-2.5 pyconll-3.1.0 sklearn-0.0 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ],
      "source": [
        "%pip install NERDA-Con"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTekGNKfVYdh",
        "outputId": "c3a52e8f-3394-4ff8-cdf8-a14fd23df6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting NERDA\n",
            "  Downloading NERDA-1.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from NERDA) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from NERDA) (1.3.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from NERDA) (4.22.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from NERDA) (0.0)\n",
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (from NERDA) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from NERDA) (3.7)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.7/dist-packages (from NERDA) (2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->NERDA) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->NERDA) (2022.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->NERDA) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->NERDA) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->NERDA) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->NERDA) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->NERDA) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->NERDA) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->NERDA) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (0.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (4.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->NERDA) (0.12.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->NERDA) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->NERDA) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->NERDA) (2022.6.15)\n",
            "Installing collected packages: NERDA\n",
            "Successfully installed NERDA-1.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install NERDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1imAG1I5hOUa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import csv\n",
        "import os\n",
        "import pyconll\n",
        "from io import BytesIO\n",
        "from itertools import compress\n",
        "from pathlib import Path\n",
        "from typing import Union, List, Dict\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "import ssl\n",
        "\n",
        "def download_unzip(url_zip: str,\n",
        "                   dir_extract: str) -> str:\n",
        "    \"\"\"Download and unzip a ZIP archive to folder.\n",
        "\n",
        "    Loads a ZIP file from URL and extracts all of the files to a\n",
        "    given folder. Does not save the ZIP file itself.\n",
        "\n",
        "    Args:\n",
        "        url_zip (str): URL to ZIP file.\n",
        "        dir_extract (str): Directory where files are extracted.\n",
        "\n",
        "    Returns:\n",
        "        str: a message telling, if the archive was succesfully\n",
        "        extracted. Obviously the files in the ZIP archive are\n",
        "        extracted to the desired directory as a side-effect.\n",
        "    \"\"\"\n",
        "\n",
        "    # suppress ssl certification\n",
        "    ctx = ssl.create_default_context()\n",
        "    ctx.check_hostname = False\n",
        "    ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    print(f'Reading {url_zip}')\n",
        "    with urlopen(url_zip, context=ctx) as zipresp:\n",
        "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
        "            zfile.extractall(dir_extract)\n",
        "\n",
        "    return f'archive extracted to {dir_extract}'\n",
        "\n",
        "def download_dane_data(dir: str = None) -> str:\n",
        "    \"\"\"Download DaNE data set.\n",
        "\n",
        "    Downloads the 'DaNE' data set annotated for Named Entity\n",
        "    Recognition developed and hosted by\n",
        "    [Alexandra Institute](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane).\n",
        "\n",
        "    Args:\n",
        "        dir (str, optional): Directory where DaNE datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory.\n",
        "\n",
        "    Returns:\n",
        "        str: a message telling, if the archive was in fact\n",
        "        succesfully extracted. Obviously the DaNE datasets are\n",
        "        extracted to the desired directory as a side-effect.\n",
        "\n",
        "    Examples:\n",
        "        >>> download_dane_data()\n",
        "        >>> download_dane_data(dir = 'DaNE')\n",
        "\n",
        "    \"\"\"\n",
        "    # set to default directory if nothing else has been provided by user.\n",
        "    if dir is None:\n",
        "        dir = os.path.join(str(Path.home()), '.dane')\n",
        "\n",
        "    return download_unzip(url_zip = 'http://danlp-downloads.alexandra.dk/datasets/ddt.zip',\n",
        "                          dir_extract = dir)\n",
        "\n",
        "def get_dane_data(split: str = 'train',\n",
        "                  limit: int = None,\n",
        "                  dir: str = None) -> dict:\n",
        "    \"\"\"Load DaNE data split.\n",
        "\n",
        "    Loads a single data split from the DaNE data set kindly hosted\n",
        "    by [Alexandra Institute](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane).\n",
        "\n",
        "    Args:\n",
        "        split (str, optional): Choose which split to load. Choose\n",
        "            from 'train', 'dev' and 'test'. Defaults to 'train'.\n",
        "        limit (int, optional): Limit the number of observations to be\n",
        "            returned from a given split. Defaults to None, which implies\n",
        "            that the entire data split is returned.\n",
        "        dir (str, optional): Directory where data is cached. If set to\n",
        "            None, the function will try to look for files in '.dane' folder in home directory.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with word-tokenized 'sentences' and named\n",
        "        entity 'tags' in IOB format.\n",
        "\n",
        "    Examples:\n",
        "        Get test split\n",
        "        >>> get_dane_data('test')\n",
        "\n",
        "        Get first 5 observations from training split\n",
        "        >>> get_dane_data('train', limit = 5)\n",
        "\n",
        "    \"\"\"\n",
        "    assert isinstance(split, str)\n",
        "    splits = ['train', 'dev', 'test']\n",
        "    assert split in splits, f'Choose between the following splits: {splits}'\n",
        "\n",
        "    # set to default directory if nothing else has been provided by user.\n",
        "    if dir is None:\n",
        "        dir = os.path.join(str(Path.home()), '.dane')\n",
        "    assert os.path.isdir(dir), f'Directory {dir} does not exist. Try downloading DaNE data with download_dane_data()'\n",
        "\n",
        "    file_path = os.path.join(dir, f'ddt.{split}.conllu')\n",
        "    assert os.path.isfile(file_path), f'File {file_path} does not exist. Try downloading DaNE data with download_dane_data()'\n",
        "\n",
        "    split = pyconll.load_from_file(file_path)\n",
        "\n",
        "    sentences = []\n",
        "    entities = []\n",
        "\n",
        "    for sent in split:\n",
        "        sentences.append([token.form for token in sent._tokens])\n",
        "        entities.append([token.misc['name'].pop() for token in sent._tokens])\n",
        "\n",
        "    if limit is not None:\n",
        "        sentences = sentences[:limit]\n",
        "        entities = entities[:limit]\n",
        "\n",
        "    return {'sentences': sentences, 'tags': entities}\n",
        "\n",
        "\n",
        "\n",
        "def download_conll_data(dir: str = None) -> str:\n",
        "    \"\"\"Download CoNLL-2003 English data set.\n",
        "\n",
        "    Downloads the [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/)\n",
        "    English data set annotated for Named Entity Recognition.\n",
        "\n",
        "    Args:\n",
        "        dir (str, optional): Directory where CoNLL-2003 datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory.\n",
        "\n",
        "    Returns:\n",
        "        str: a message telling, if the archive was in fact\n",
        "        succesfully extracted. Obviously the CoNLL datasets are\n",
        "        extracted to the desired directory as a side-effect.\n",
        "\n",
        "    Examples:\n",
        "        >>> download_conll_data()\n",
        "        >>> download_conll_data(dir = 'conll')\n",
        "\n",
        "    \"\"\"\n",
        "    # set to default directory if nothing else has been provided by user.\n",
        "    if dir is None:\n",
        "        dir = os.path.join(str(Path.home()), '.conll')\n",
        "\n",
        "    return download_unzip(url_zip = 'https://data.deepai.org/conll2003.zip',\n",
        "                          dir_extract = dir)\n",
        "\n",
        "def get_conll_data(split: str = 'train',\n",
        "                   limit: int = None,\n",
        "                   dir: str = None) -> dict:\n",
        "    \"\"\"Load CoNLL-2003 (English) data split.\n",
        "\n",
        "    Loads a single data split from the\n",
        "    [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/)\n",
        "    (English) data set.\n",
        "\n",
        "    Args:\n",
        "        split (str, optional): Choose which split to load. Choose\n",
        "            from 'train', 'valid' and 'test'. Defaults to 'train'.\n",
        "        limit (int, optional): Limit the number of observations to be\n",
        "            returned from a given split. Defaults to None, which implies\n",
        "            that the entire data split is returned.\n",
        "        dir (str, optional): Directory where data is cached. If set to\n",
        "            None, the function will try to look for files in '.conll' folder in home directory.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with word-tokenized 'sentences' and named\n",
        "        entity 'tags' in IOB format.\n",
        "\n",
        "    Examples:\n",
        "        Get test split\n",
        "        >>> get_conll_data('test')\n",
        "\n",
        "        Get first 5 observations from training split\n",
        "        >>> get_conll_data('train', limit = 5)\n",
        "\n",
        "    \"\"\"\n",
        "    assert isinstance(split, str)\n",
        "    splits = ['train', 'valid', 'test']\n",
        "    assert split in splits, f'Choose between the following splits: {splits}'\n",
        "\n",
        "    # set to default directory if nothing else has been provided by user.\n",
        "    if dir is None:\n",
        "        dir = os.path.join(str(Path.home()), '.conll')\n",
        "    assert os.path.isdir(dir), f'Directory {dir} does not exist. Try downloading CoNLL-2003 data with download_conll_data()'\n",
        "\n",
        "    file_path = os.path.join(dir, f'{split}.txt')\n",
        "    assert os.path.isfile(file_path), f'File {file_path} does not exist. Try downloading CoNLL-2003 data with download_conll_data()'\n",
        "\n",
        "    # read data from file.\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        reader = csv.reader(file, delimiter = ' ')\n",
        "        for row in reader:\n",
        "            data.append([row])\n",
        "\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    entities = []\n",
        "    tags = []\n",
        "\n",
        "    for row in data:\n",
        "        # extract first element of list.\n",
        "        row = row[0]\n",
        "        # TO DO: move to data reader.\n",
        "        if len(row) > 0 and row[0] != '-DOCSTART-':\n",
        "            sentence.append(row[0])\n",
        "            tags.append(row[-1])\n",
        "        if len(row) == 0 and len(sentence) > 0:\n",
        "            # clean up sentence/tags.\n",
        "            # remove white spaces.\n",
        "            selector = [word != ' ' for word in sentence]\n",
        "            sentence = list(compress(sentence, selector))\n",
        "            tags = list(compress(tags, selector))\n",
        "            # append if sentence length is still greater than zero..\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "                entities.append(tags)\n",
        "            sentence = []\n",
        "            tags = []\n",
        "\n",
        "\n",
        "    if limit is not None:\n",
        "        sentences = sentences[:limit]\n",
        "        entities = entities[:limit]\n",
        "\n",
        "    return {'sentences': sentences, 'tags': entities}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqmRzcGkh6jz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This section covers the interface for `NERDA` models, that is\n",
        "implemented as its own Python class [NERDA.models.NERDA][].\n",
        "\n",
        "The interface enables you to easily\n",
        "\n",
        "- specify your own [NERDA.models.NERDA][] model\n",
        "- train it\n",
        "- evaluate it\n",
        "- use it to predict entities in new texts.\n",
        "\"\"\"\n",
        "from NERDA_Con.datasets import get_conll_data\n",
        "from NERDA_Con.networks import NERDANetwork\n",
        "from NERDA_Con.predictions import predict, predict_text\n",
        "from NERDA_Con.performance import compute_f1_scores, flatten\n",
        "from NERDA_Con.training import train_model, train_model_new_task\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import sklearn.preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from typing import List\n",
        "\n",
        "class NERDA:\n",
        "\n",
        "    def __init__(self,\n",
        "                 transformer: str = 'bert-base-multilingual-uncased',\n",
        "                 device: str = None,\n",
        "                 tag_scheme: List[str] = [\n",
        "                            'B-PER',\n",
        "                            'I-PER',\n",
        "                            'B-ORG',\n",
        "                            'I-ORG',\n",
        "                            'B-LOC',\n",
        "                            'I-LOC',\n",
        "                            'B-MISC',\n",
        "                            'I-MISC'\n",
        "                            ],\n",
        "                 tag_outside: str = 'O',\n",
        "                 dataset_training: dict = None,\n",
        "                 dataset_validation: dict = None,\n",
        "                 max_len: int = 128,\n",
        "                 network: torch.nn.Module = NERDANetwork,\n",
        "                 dropout: float = 0.1,\n",
        "                 hyperparameters: dict = {'epochs' : 4,\n",
        "                                          'warmup_steps' : 500,\n",
        "                                          'train_batch_size': 13,\n",
        "                                          'learning_rate': 0.0001},\n",
        "                 tokenizer_parameters: dict = {'do_lower_case' : True},\n",
        "                 validation_batch_size: int = 8,\n",
        "                 num_workers: int = 1) -> None:\n",
        "\n",
        "\n",
        "        # set device automatically if not provided by user.\n",
        "        if device is None:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(\"Device automatically set to:\", self.device)\n",
        "        else:\n",
        "            self.device = device\n",
        "            print(\"Device set to:\", self.device)\n",
        "        self.tag_scheme = tag_scheme\n",
        "        self.tag_outside = tag_outside\n",
        "        self.transformer = transformer\n",
        "        self.dataset_training = dataset_training\n",
        "        self.dataset_validation = dataset_validation\n",
        "        self.hyperparameters = hyperparameters\n",
        "        self.tag_outside = tag_outside\n",
        "        self.tag_scheme = tag_scheme\n",
        "        tag_complete = [tag_outside] + tag_scheme\n",
        "        # fit encoder to _all_ possible tags.\n",
        "        self.max_len = max_len\n",
        "        self.tag_encoder = sklearn.preprocessing.LabelEncoder()\n",
        "        self.tag_encoder.fit(tag_complete)\n",
        "        self.transformer_model = AutoModel.from_pretrained(transformer)\n",
        "        self.transformer_tokenizer = AutoTokenizer.from_pretrained(transformer, **tokenizer_parameters)\n",
        "        self.transformer_config = AutoConfig.from_pretrained(transformer)\n",
        "        self.network = NERDANetwork(self.transformer_model, self.device, len(tag_complete), dropout = dropout)\n",
        "        self.network.to(self.device)\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.train_losses = []\n",
        "        self.valid_loss = np.nan\n",
        "        self.quantized = False\n",
        "        self.halved = False\n",
        "        self.fisher_dict= {}\n",
        "        self.opt_param_dict = {}\n",
        "        self.task_id = 0\n",
        "        self.shared_model = None\n",
        "\n",
        "    def train(self) -> str:\n",
        "        \"\"\"Train Network\n",
        "\n",
        "        Trains the network from the NERDA model specification.\n",
        "\n",
        "        Returns:\n",
        "            str: a message saying if the model was trained succesfully.\n",
        "            The network in the 'network' attribute is trained as a\n",
        "            side-effect. Training losses and validation loss are saved\n",
        "            in 'training_losses' and 'valid_loss'\n",
        "            attributes respectively as side-effects.\n",
        "        \"\"\"\n",
        "        network, train_losses, valid_loss = train_model(network = self.network,\n",
        "                                                        tag_encoder = self.tag_encoder,\n",
        "                                                        tag_outside = self.tag_outside,\n",
        "                                                        transformer_tokenizer = self.transformer_tokenizer,\n",
        "                                                        transformer_config = self.transformer_config,\n",
        "                                                        dataset_training = self.dataset_training,\n",
        "                                                        dataset_validation = self.dataset_validation,\n",
        "                                                        validation_batch_size = self.validation_batch_size,\n",
        "                                                        max_len = self.max_len,\n",
        "                                                        device = self.device,\n",
        "                                                        num_workers = self.num_workers,\n",
        "                                                        **self.hyperparameters)\n",
        "\n",
        "        # attach as attributes to class\n",
        "        setattr(self, \"network\", network)\n",
        "        setattr(self, \"train_losses\", train_losses)\n",
        "        setattr(self, \"valid_loss\", valid_loss)\n",
        "\n",
        "        return \"Model trained successfully\"\n",
        "\n",
        "    def train_next_task(self, new_dataset_training, new_dataset_validation) -> str:\n",
        "        \"\"\"Train Network\n",
        "\n",
        "        Trains the network from the NERDA model specification.\n",
        "\n",
        "        Returns:\n",
        "            str: a message saying if the model was trained succesfully.\n",
        "            The network in the 'network' attribute is trained as a\n",
        "            side-effect. Training losses and validation loss are saved\n",
        "            in 'training_losses' and 'valid_loss'\n",
        "            attributes respectively as side-effects.\n",
        "        \"\"\"\n",
        "        network, train_losses, valid_loss = train_model_new_task(network = self.network,\n",
        "                                                        tag_encoder = self.tag_encoder,\n",
        "                                                        tag_outside = self.tag_outside,\n",
        "                                                        transformer_tokenizer = self.transformer_tokenizer,\n",
        "                                                        transformer_config = self.transformer_config,\n",
        "                                                        dataset_training = new_dataset_training,\n",
        "                                                        dataset_validation = new_dataset_validation,\n",
        "                                                        validation_batch_size = self.validation_batch_size,\n",
        "                                                        max_len = self.max_len,\n",
        "                                                        device = self.device,\n",
        "                                                        num_workers = self.num_workers,\n",
        "                                                        task_id = self.task_id,\n",
        "                                                        fisher_dict=self.fisher_dict,\n",
        "                                                        opt_param_dict = self.opt_param_dict,\n",
        "                                                        shared_model = self.shared_model,\n",
        "                                                        **self.hyperparameters)\n",
        "\n",
        "        # attach as attributes to class\n",
        "        setattr(self, \"network\", network)\n",
        "        setattr(self, \"train_losses\", train_losses)\n",
        "        setattr(self, \"valid_loss\", valid_loss)\n",
        "        self.task_id += 1\n",
        "\n",
        "        return \"Model trained successfully\"\n",
        "\n",
        "\n",
        "    def load_network_from_file(self, model_path = \"model.bin\") -> str:\n",
        "        \"\"\"Load Pretrained NERDA Network from file\n",
        "\n",
        "        Loads weights for a pretrained NERDA Network from file.\n",
        "\n",
        "        Args:\n",
        "            model_path (str, optional): Path for model file.\n",
        "                Defaults to \"model.bin\".\n",
        "\n",
        "        Returns:\n",
        "            str: message telling if weights for network were\n",
        "            loaded succesfully.\n",
        "        \"\"\"\n",
        "        # TODO: change assert to Raise.\n",
        "        assert os.path.exists(model_path), \"File does not exist. You can download network with download_network()\"\n",
        "        self.network.load_state_dict(torch.load(model_path, map_location = torch.device(self.device)))\n",
        "        self.network.device = self.device\n",
        "        return f'Weights for network loaded from {model_path}'\n",
        "\n",
        "    def save_network(self, model_path:str = \"model.bin\") -> None:\n",
        "        \"\"\"Save Weights of NERDA Network\n",
        "\n",
        "        Saves weights for a fine-tuned NERDA Network to file.\n",
        "\n",
        "        Args:\n",
        "            model_path (str, optional): Path for model file.\n",
        "                Defaults to \"model.bin\".\n",
        "\n",
        "        Returns:\n",
        "            Nothing. Saves model to file as a side-effect.\n",
        "        \"\"\"\n",
        "        torch.save(self.network.state_dict(), model_path)\n",
        "        print(f\"Network written to file {model_path}\")\n",
        "\n",
        "    def quantize(self):\n",
        "        \"\"\"Apply dynamic quantization to increase performance.\n",
        "\n",
        "        Quantization and half precision inference are mutually exclusive.\n",
        "\n",
        "        Read more: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html\n",
        "\n",
        "        Returns:\n",
        "            Nothing. Applies dynamic quantization to Network as a side-effect.\n",
        "        \"\"\"\n",
        "        assert not (self.quantized), \"Dynamic quantization already applied\"\n",
        "        assert not (self.halved), \"Can't run both quantization and half precision\"\n",
        "\n",
        "        self.network = torch.quantization.quantize_dynamic(\n",
        "            self.network, {torch.nn.Linear}, dtype=torch.qint8\n",
        "        )\n",
        "        self.quantized = True\n",
        "\n",
        "    def half(self):\n",
        "        \"\"\"Convert weights from Float32 to Float16 to increase performance\n",
        "\n",
        "        Quantization and half precision inference are mutually exclusive.\n",
        "\n",
        "        Read more: https://pytorch.org/docs/master/generated/torch.nn.Module.html?highlight=half#torch.nn.Module.half\n",
        "\n",
        "        Returns:\n",
        "            Nothing. Model is \"halved\" as a side-effect.\n",
        "        \"\"\"\n",
        "        assert not (self.halved), \"Half precision already applied\"\n",
        "        assert not (self.quantized), \"Can't run both quantization and half precision\"\n",
        "\n",
        "        self.network.half()\n",
        "        self.halved = True\n",
        "\n",
        "    def predict(self, sentences: List[List[str]],\n",
        "                return_confidence: bool = False,\n",
        "                **kwargs) -> List[List[str]]:\n",
        "        \"\"\"Predict Named Entities in Word-Tokenized Sentences\n",
        "\n",
        "        Predicts word-tokenized sentences with trained model.\n",
        "\n",
        "        Args:\n",
        "            sentences (List[List[str]]): word-tokenized sentences.\n",
        "            kwargs: arbitrary keyword arguments. For instance\n",
        "                'batch_size' and 'num_workers'.\n",
        "            return_confidence (bool, optional): if True, return\n",
        "                confidence scores for all predicted tokens. Defaults\n",
        "                to False.\n",
        "\n",
        "        Returns:\n",
        "            List[List[str]]: Predicted tags for sentences - one\n",
        "            predicted tag/entity per word token.\n",
        "        \"\"\"\n",
        "        return predict(network = self.network,\n",
        "                       sentences = sentences,\n",
        "                       transformer_tokenizer = self.transformer_tokenizer,\n",
        "                       transformer_config = self.transformer_config,\n",
        "                       max_len = self.max_len,\n",
        "                       device = self.device,\n",
        "                       tag_encoder = self.tag_encoder,\n",
        "                       tag_outside = self.tag_outside,\n",
        "                       return_confidence = return_confidence,\n",
        "                       **kwargs)\n",
        "\n",
        "    def predict_text(self, text: str,\n",
        "                     return_confidence:bool = False, **kwargs) -> list:\n",
        "        \"\"\"Predict Named Entities in a Text\n",
        "\n",
        "        Args:\n",
        "            text (str): text to predict entities in.\n",
        "            kwargs: arbitrary keyword arguments. For instance\n",
        "                'batch_size' and 'num_workers'.\n",
        "            return_confidence (bool, optional): if True, return\n",
        "                confidence scores for all predicted tokens. Defaults\n",
        "                to False.\n",
        "\n",
        "        Returns:\n",
        "            tuple: word-tokenized sentences and predicted\n",
        "            tags/entities.\n",
        "        \"\"\"\n",
        "        return predict_text(network = self.network,\n",
        "                            text = text,\n",
        "                            transformer_tokenizer = self.transformer_tokenizer,\n",
        "                            transformer_config = self.transformer_config,\n",
        "                            max_len = self.max_len,\n",
        "                            device = self.device,\n",
        "                            tag_encoder = self.tag_encoder,\n",
        "                            tag_outside = self.tag_outside,\n",
        "                            return_confidence=return_confidence,\n",
        "                            **kwargs)\n",
        "\n",
        "    def evaluate_performance(self, dataset: dict,\n",
        "                             return_accuracy: bool=False,\n",
        "                             **kwargs) -> pd.DataFrame:\n",
        "        \"\"\"Evaluate Performance\n",
        "\n",
        "        Evaluates the performance of the model on an arbitrary\n",
        "        data set.\n",
        "\n",
        "        Args:\n",
        "            dataset (dict): Data set that must consist of\n",
        "                'sentences' and NER'tags'. You can look at examples\n",
        "                 of, how the dataset should look like by invoking functions\n",
        "                 get_dane_data() or get_conll_data().\n",
        "            kwargs: arbitrary keyword arguments for predict. For\n",
        "                instance 'batch_size' and 'num_workers'.\n",
        "            return_accuracy (bool): Return accuracy\n",
        "                as well? Defaults to False.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with performance numbers, F1-scores,\n",
        "            Precision and Recall. Returns dictionary with\n",
        "            this AND accuracy, if return_accuracy is set to\n",
        "            True.\n",
        "        \"\"\"\n",
        "\n",
        "        tags_predicted = self.predict(dataset.get('sentences'),\n",
        "                                      **kwargs)\n",
        "\n",
        "        # compute F1 scores by entity type\n",
        "        f1 = compute_f1_scores(y_pred = tags_predicted,\n",
        "                               y_true = dataset.get('tags'),\n",
        "                               labels = self.tag_scheme,\n",
        "                               average = None)\n",
        "\n",
        "        # create DataFrame with performance scores (=F1)\n",
        "        df = list(zip(self.tag_scheme, f1[2], f1[0], f1[1]))\n",
        "        df = pd.DataFrame(df, columns = ['Level', 'F1-Score', 'Precision', 'Recall'])\n",
        "\n",
        "        # compute MICRO-averaged F1-scores and add to table.\n",
        "        f1_micro = compute_f1_scores(y_pred = tags_predicted,\n",
        "                                     y_true = dataset.get('tags'),\n",
        "                                     labels = self.tag_scheme,\n",
        "                                     average = 'micro')\n",
        "        f1_micro = pd.DataFrame({'Level' : ['AVG_MICRO'],\n",
        "                                 'F1-Score': [f1_micro[2]],\n",
        "                                 'Precision': [np.nan],\n",
        "                                 'Recall': [np.nan]})\n",
        "        df = df.append(f1_micro)\n",
        "\n",
        "        # compute MACRO-averaged F1-scores and add to table.\n",
        "        f1_macro = compute_f1_scores(y_pred = tags_predicted,\n",
        "                                     y_true = dataset.get('tags'),\n",
        "                                     labels = self.tag_scheme,\n",
        "                                     average = 'macro')\n",
        "        f1_macro = pd.DataFrame({'Level' : ['AVG_MICRO'],\n",
        "                                 'F1-Score': [f1_macro[2]],\n",
        "                                 'Precision': [np.nan],\n",
        "                                 'Recall': [np.nan]})\n",
        "        df = df.append(f1_macro)\n",
        "\n",
        "        # compute and return accuracy if desired\n",
        "        if return_accuracy:\n",
        "            accuracy = accuracy_score(y_pred = flatten(tags_predicted),\n",
        "                                      y_true = flatten(dataset.get('tags')))\n",
        "            return {'f1':df, 'accuracy': accuracy}\n",
        "\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUI3zHsfiL5w"
      },
      "outputs": [],
      "source": [
        "\"\"\"This section covers `torch` networks for `NERDA`\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig\n",
        "from NERDA_Con.utils import match_kwargs\n",
        "\n",
        "class NERDANetwork(nn.Module):\n",
        "    \"\"\"A Generic Network for NERDA models.\n",
        "\n",
        "    The network has an analogous architecture to the models in\n",
        "    [Hvingelby et al. 2020](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.565.pdf).\n",
        "\n",
        "    Can be replaced with a custom user-defined network with\n",
        "    the restriction, that it must take the same arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transformer: nn.Module, device: str, n_tags: int, dropout: float = 0.1) -> None:\n",
        "        \"\"\"Initialize a NERDA Network\n",
        "\n",
        "        Args:\n",
        "            transformer (nn.Module): huggingface `torch` transformer.\n",
        "            device (str): Computational device.\n",
        "            n_tags (int): Number of unique entity tags (incl. outside tag)\n",
        "            dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        super(NERDANetwork, self).__init__()\n",
        "\n",
        "        # extract transformer name\n",
        "        transformer_name = transformer.name_or_path\n",
        "        # extract AutoConfig, from which relevant parameters can be extracted.\n",
        "        transformer_config = AutoConfig.from_pretrained(transformer_name)\n",
        "\n",
        "        self.transformer = transformer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.tags = nn.Linear(transformer_config.hidden_size, n_tags)\n",
        "        self.device = device\n",
        "\n",
        "    # NOTE: 'offsets 'are not used in model as-is, but they are expected as output\n",
        "    # down-stream. So _DON'T_ remove! :)\n",
        "    def forward(self,\n",
        "                input_ids: torch.Tensor,\n",
        "                masks: torch.Tensor,\n",
        "                token_type_ids: torch.Tensor,\n",
        "                target_tags: torch.Tensor,\n",
        "                offsets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Model Forward Iteration\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input IDs.\n",
        "            masks (torch.Tensor): Attention Masks.\n",
        "            token_type_ids (torch.Tensor): Token Type IDs.\n",
        "            target_tags (torch.Tensor): Target tags. Are not used\n",
        "                in model as-is, but they are expected downstream,\n",
        "                so they can not be left out.\n",
        "            offsets (torch.Tensor): Offsets to keep track of original\n",
        "                words. Are not used in model as-is, but they are\n",
        "                expected as down-stream, so they can not be left out.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: predicted values.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: can be improved with ** and move everything to device in a\n",
        "        # single step.\n",
        "        transformer_inputs = {\n",
        "            'input_ids': input_ids.to(self.device),\n",
        "            'masks': masks.to(self.device),\n",
        "            'token_type_ids': token_type_ids.to(self.device)\n",
        "            }\n",
        "\n",
        "        # match args with transformer\n",
        "        transformer_inputs = match_kwargs(self.transformer.forward, **transformer_inputs)\n",
        "\n",
        "        outputs = self.transformer(**transformer_inputs)[0]\n",
        "\n",
        "        # apply drop-out\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # outputs for all labels/tags\n",
        "        outputs = self.tags(outputs)\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5-_CDQ9iPYC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This section covers functionality for computing performance\n",
        "for [NERDA.models.NERDA][] models.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import warnings\n",
        "\n",
        "def flatten(l: list):\n",
        "    \"\"\"Flattens list\"\"\"\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "\n",
        "def compute_f1_scores(y_pred: List[List[str]],\n",
        "                      y_true: List[List[str]],\n",
        "                      labels: List[str],\n",
        "                      **kwargs) -> list:\n",
        "    \"\"\"Compute F1 scores.\n",
        "\n",
        "    Computes F1 Scores\n",
        "\n",
        "    Args:\n",
        "        y_pred (List): predicted values.\n",
        "        y_true (List): observed/true values.\n",
        "        labels (List): all possible tags.\n",
        "        kwargs: all optional arguments for precision/recall function.\n",
        "\n",
        "    Returns:\n",
        "        list: resulting F1 scores.\n",
        "\n",
        "    \"\"\"\n",
        "    # check inputs.\n",
        "    assert sum([len(t) < len(p) for t, p in zip(y_true, y_pred)]) == 0, \"Length of predictions must not exceed length of observed values\"\n",
        "\n",
        "    # check, if some lengths of observed values exceed predicted values.\n",
        "    n_exceeds = sum([len(t) > len(p) for t, p in zip(y_true, y_pred)])\n",
        "    if n_exceeds > 0:\n",
        "        warnings.warn(f'length of observed values exceeded lengths of predicted values in {n_exceeds} cases and were truncated. _Consider_ increasing max_len parameter for your model.')\n",
        "\n",
        "    # truncate observed values dimensions to match predicted values,\n",
        "    # this is needed if predictions have been truncated earlier in\n",
        "    # the flow.\n",
        "    y_true = [t[:len(p)] for t, p in zip(y_true, y_pred)]\n",
        "\n",
        "    y_pred = flatten(y_pred)\n",
        "    y_true = flatten(y_true)\n",
        "\n",
        "    f1_scores = precision_recall_fscore_support(y_true = y_true,\n",
        "                                                y_pred = y_pred,\n",
        "                                                labels = labels,\n",
        "                                                **kwargs)\n",
        "\n",
        "    return f1_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EgkjUYziSfe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This sections covers NERDA Models that have been 'precooked' by\n",
        "Ekstra Bladet and are publicly available for download.\n",
        "\"\"\"\n",
        "from NERDA_Con.datasets import get_dane_data, get_conll_data\n",
        "from NERDA_Con.models import NERDA\n",
        "import os\n",
        "import urllib\n",
        "from pathlib import Path\n",
        "from progressbar import ProgressBar\n",
        "\n",
        "pbar = None\n",
        "\n",
        "# helper function to show progressbar\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = ProgressBar(maxval=total_size)\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    pbar.start()\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "class Precooked(NERDA):\n",
        "    \"\"\"Precooked NERDA Model\n",
        "\n",
        "    NERDA model specification that has been precooked/pretrained\n",
        "    and is available for download.\n",
        "\n",
        "    Inherits from [NERDA.models.NERDA][].\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        \"\"\"Initialize Precooked NERDA Model\n",
        "\n",
        "        Args:\n",
        "            kwargs: all arguments for NERDA Model.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def download_network(self, dir = None) -> None:\n",
        "        \"\"\"Download Precooked Network from Web\n",
        "\n",
        "        Args:\n",
        "            dir (str, optional): Directory where the model file\n",
        "                will be saved. Defaults to None, in which case\n",
        "                the model will be saved in a folder '.nerda' in\n",
        "                your home directory.\n",
        "\n",
        "        Returns:\n",
        "            str: Message saying if the download was successfull.\n",
        "            Model is downloaded as a side-effect.\n",
        "        \"\"\"\n",
        "\n",
        "        model_name = type(self).__name__\n",
        "\n",
        "        # url for public S3 bucket with NERDA models.\n",
        "        url_s3 = 'https://nerda.s3-eu-west-1.amazonaws.com'\n",
        "        url_model = f'{url_s3}/{model_name}.bin'\n",
        "\n",
        "        if dir is None:\n",
        "            dir = os.path.join(str(Path.home()), '.nerda')\n",
        "\n",
        "        if not os.path.exists(dir):\n",
        "            os.mkdir(dir)\n",
        "\n",
        "        file_path = os.path.join(dir, f'{model_name}.bin')\n",
        "\n",
        "        print(\n",
        "        \"\"\"\n",
        "        Please make sure, that you're running the latest version of 'NERDA'\n",
        "        otherwise the model is not guaranteed to work.\n",
        "        \"\"\"\n",
        "        )\n",
        "        print(f'Downloading {url_model} to {file_path}')\n",
        "        urllib.request.urlretrieve(url_model, file_path, show_progress)\n",
        "\n",
        "        return \"Network downloaded successfully. Load network with 'load_network'.\"\n",
        "\n",
        "    def load_network(self, file_path: str = None) -> None:\n",
        "        \"\"\"Load Pretrained Network\n",
        "\n",
        "        Loads pretrained network from file.\n",
        "\n",
        "        Args:\n",
        "            file_path (str, optional): Path to model file. Defaults to None,\n",
        "                in which case, the function points to the '.nerda' folder\n",
        "                the home directory.\n",
        "        \"\"\"\n",
        "\n",
        "        model_name = type(self).__name__\n",
        "\n",
        "        if file_path is None:\n",
        "            file_path = os.path.join(str(Path.home()), '.nerda', f'{model_name}.bin')\n",
        "\n",
        "        assert os.path.exists(file_path), \"File does not exist! You can download network with download_network()\"\n",
        "        print(\n",
        "        \"\"\"\n",
        "        Model loaded. Please make sure, that you're running the latest version\n",
        "        of 'NERDA' otherwise the model is not guaranteed to work.\n",
        "        \"\"\"\n",
        "        )\n",
        "        self.load_network_from_file(file_path)\n",
        "\n",
        "class DA_BERT_ML(Precooked):\n",
        "    \"\"\"NERDA [Multilingual BERT](https://huggingface.co/bert-base-multilingual-uncased)\n",
        "    for Danish Finetuned on [DaNE data set](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane).\n",
        "\n",
        "    Inherits from [NERDA.precooked.Precooked][].\n",
        "\n",
        "    Examples:\n",
        "        >>> from NERDA.precooked import DA_BERT_ML()\n",
        "        >>> model = DA_BERT_ML()\n",
        "        >>> model.download_network()\n",
        "        >>> model.load_network()\n",
        "        >>> text = 'Jens Hansen har en bondegård'\n",
        "        >>> model.predict_text(text)\n",
        "        ([['Jens', 'Hansen', 'har', 'en', 'bondegård']], [['B-PER', 'I-PER', 'O', 'O', 'O']])\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, device: str = None) -> None:\n",
        "        \"\"\"Initialize model\"\"\"\n",
        "        super().__init__(transformer = 'bert-base-multilingual-uncased',\n",
        "                         device = device,\n",
        "                         tag_scheme = [\n",
        "                            'B-PER',\n",
        "                            'I-PER',\n",
        "                            'B-ORG',\n",
        "                            'I-ORG',\n",
        "                            'B-LOC',\n",
        "                            'I-LOC',\n",
        "                            'B-MISC',\n",
        "                            'I-MISC'\n",
        "                            ],\n",
        "                         tag_outside = 'O',\n",
        "                         max_len = 128,\n",
        "                         dropout = 0.1,\n",
        "                         hyperparameters = {'epochs' : 4,\n",
        "                                            'warmup_steps' : 500,\n",
        "                                            'train_batch_size': 13,\n",
        "                                            'learning_rate': 0.0001},\n",
        "                         tokenizer_parameters = {'do_lower_case' : True})\n",
        "\n",
        "class DA_DISTILBERT_ML(Precooked):\n",
        "    \"\"\"NERDA [Multilingual BERT](https://huggingface.co/distilbert-base-multilingual-cased)\n",
        "    for Danish Finetuned on [DaNE data set](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane).\n",
        "\n",
        "    Inherits from [NERDA.precooked.Precooked][].\n",
        "\n",
        "    Examples:\n",
        "        >>> from NERDA.precooked import DA_DISTILBERT_ML()\n",
        "        >>> model = DA_DISTILBERT_ML()\n",
        "        >>> model.download_network()\n",
        "        >>> model.load_network()\n",
        "        >>> text = 'Jens Hansen har en bondegård'\n",
        "        >>> model.predict_text(text)\n",
        "        ([['Jens', 'Hansen', 'har', 'en', 'bondegård']], [['B-PER', 'I-PER', 'O', 'O', 'O']])\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, device: str = None) -> None:\n",
        "        \"\"\"Initialize model\"\"\"\n",
        "        super().__init__(transformer = 'distilbert-base-multilingual-cased',\n",
        "                         device = device,\n",
        "                         tag_scheme = [\n",
        "                            'B-PER',\n",
        "                            'I-PER',\n",
        "                            'B-ORG',\n",
        "                            'I-ORG',\n",
        "                            'B-LOC',\n",
        "                            'I-LOC',\n",
        "                            'B-MISC',\n",
        "                            'I-MISC'\n",
        "                            ],\n",
        "                         tag_outside = 'O',\n",
        "                         max_len = 128,\n",
        "                         dropout = 0.1,\n",
        "                         hyperparameters = {'epochs' : 4,\n",
        "                                            'warmup_steps' : 500,\n",
        "                                            'train_batch_size': 13,\n",
        "                                            'learning_rate': 0.0001},\n",
        "                         tokenizer_parameters = {'do_lower_case' : False})\n",
        "\n",
        "class DA_ELECTRA_DA(Precooked):\n",
        "    \"\"\"NERDA [Danish ELECTRA](https://huggingface.co/Maltehb/-l-ctra-danish-electra-small-uncased)\n",
        "    for Danish finetuned on [DaNE data set](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane).\n",
        "\n",
        "    Inherits from [NERDA.precooked.Precooked][].\n",
        "\n",
        "    Examples:\n",
        "        >>> from NERDA.precooked import DA_ELECTRA_DA()\n",
        "        >>> model = DA_ELECTRA_DA()\n",
        "        >>> model.download_network()\n",
        "        >>> model.load_network()\n",
        "        >>> text = 'Jens Hansen har en bondegård'\n",
        "        >>> model.predict_text(text)\n",
        "        ([['Jens', 'Hansen', 'har', 'en', 'bondegård']], [['B-PER', 'I-PER', 'O', 'O', 'O']])\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, device: str = None) -> None:\n",
        "        \"\"\"Initialize model\"\"\"\n",
        "        super().__init__(transformer = 'Maltehb/-l-ctra-danish-electra-small-uncased',\n",
        "                         device = device,\n",
        "                         tag_scheme = [\n",
        "                            'B-PER',\n",
        "                            'I-PER',\n",
        "                            'B-ORG',\n",
        "                            'I-ORG',\n",
        "                            'B-LOC',\n",
        "                            'I-LOC',\n",
        "                            'B-MISC',\n",
        "                            'I-MISC'\n",
        "                            ],\n",
        "                         tag_outside = 'O',\n",
        "                         max_len = 128,\n",
        "                         dropout = 0.1,\n",
        "                         hyperparameters = {'epochs' : 5,\n",
        "                                            'warmup_steps' : 500,\n",
        "                                            'train_batch_size': 13,\n",
        "                                            'learning_rate': 0.0001},\n",
        "                         tokenizer_parameters = {'do_lower_case' : True})\n",
        "\n",
        "class EN_ELECTRA_EN(Precooked):\n",
        "    \"\"\"NERDA [English ELECTRA](https://huggingface.co/google/electra-small-discriminator)\n",
        "    for English finetuned on [CoNLL-2003 data set](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
        "\n",
        "    Inherits from [NERDA.precooked.Precooked][].\n",
        "\n",
        "    Examples:\n",
        "        >>> from NERDA.precooked import EN_ELECTRA_EN()\n",
        "        >>> model = EN_ELECTRA_EN()\n",
        "        >>> model.download_network()\n",
        "        >>> model.load_network()\n",
        "        >>> text = 'Old MacDonald had a farm'\n",
        "        >>> model.predict_text(text)\n",
        "        ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['B-PER', 'I-PER', 'O', 'O', 'O']])\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, device: str = None) -> None:\n",
        "        \"\"\"Initialize model\"\"\"\n",
        "        super().__init__(transformer = 'google/electra-small-discriminator',\n",
        "                         device = device,\n",
        "                         tag_scheme = [\n",
        "                            'B-PER',\n",
        "                            'I-PER',\n",
        "                            'B-ORG',\n",
        "                            'I-ORG',\n",
        "                            'B-LOC',\n",
        "                            'I-LOC',\n",
        "                            'B-MISC',\n",
        "                            'I-MISC'\n",
        "                            ],\n",
        "                         tag_outside = 'O',\n",
        "                         max_len = 128,\n",
        "                         dropout = 0.1,\n",
        "                         hyperparameters = {'epochs' : 4,\n",
        "                                            'warmup_steps' : 250,\n",
        "                                            'train_batch_size': 13,\n",
        "                                            'learning_rate': 8e-05},\n",
        "                         tokenizer_parameters = {'do_lower_case' : True})\n",
        "\n",
        "\n",
        "class EN_BERT_ML(Precooked):\n",
        "    \"\"\"NERDA [Multilingual BERT](https://huggingface.co/bert-base-multilingual-uncased)\n",
        "    for English finetuned on [CoNLL-2003 data set](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
        "\n",
        "    Inherits from [NERDA.precooked.Precooked][].\n",
        "\n",
        "    Examples:\n",
        "        >>> from NERDA.precooked import EN_BERT_ML()\n",
        "        >>> model = EN_BERT_ML()\n",
        "        >>> model.download_network()\n",
        "        >>> model.load_network()\n",
        "        >>> text = 'Old MacDonald had a farm'\n",
        "        >>> model.predict_text(text)\n",
        "        ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['B-PER', 'I-PER', 'O', 'O', 'O']])\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, device: str = None) -> None:\n",
        "        \"\"\"Initialize model\"\"\"\n",
        "        super().__init__(transformer = 'bert-base-multilingual-uncased',\n",
        "                         device = device,\n",
        "                         tag_scheme = [\n",
        "                            'B-PER',\n",
        "                            'I-PER',\n",
        "                            'B-ORG',\n",
        "                            'I-ORG',\n",
        "                            'B-LOC',\n",
        "                            'I-LOC',\n",
        "                            'B-MISC',\n",
        "                            'I-MISC'\n",
        "                            ],\n",
        "                         tag_outside = 'O',\n",
        "                         max_len = 128,\n",
        "                         dropout = 0.1,\n",
        "                         hyperparameters = {'epochs' : 4,\n",
        "                                            'warmup_steps' : 500,\n",
        "                                            'train_batch_size': 13,\n",
        "                                            'learning_rate': 0.0001},\n",
        "                         tokenizer_parameters = {'do_lower_case' : True})\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih1AFeXgiXxE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This section covers functionality for computing predictions\n",
        "with a [NERDA.models.NERDA][] model.\n",
        "\"\"\"\n",
        "\n",
        "from NERDA_Con.preprocessing import create_dataloader\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from typing import List, Callable\n",
        "import transformers\n",
        "import sklearn.preprocessing\n",
        "\n",
        "def sigmoid_transform(x):\n",
        "    prob = 1/(1 + np.exp(-x))\n",
        "    return prob\n",
        "\n",
        "def predict(network: torch.nn.Module,\n",
        "            sentences: List[List[str]],\n",
        "            transformer_tokenizer: transformers.PreTrainedTokenizer,\n",
        "            transformer_config: transformers.PretrainedConfig,\n",
        "            max_len: int,\n",
        "            device: str,\n",
        "            tag_encoder: sklearn.preprocessing.LabelEncoder,\n",
        "            tag_outside: str,\n",
        "            batch_size: int = 8,\n",
        "            num_workers: int = 1,\n",
        "            return_tensors: bool = False,\n",
        "            return_confidence: bool = False,\n",
        "            pad_sequences: bool = True) -> List[List[str]]:\n",
        "    \"\"\"Compute predictions.\n",
        "\n",
        "    Computes predictions for a list with word-tokenized sentences\n",
        "    with a `NERDA` model.\n",
        "\n",
        "    Args:\n",
        "        network (torch.nn.Module): Network.\n",
        "        sentences (List[List[str]]): List of lists with word-tokenized\n",
        "            sentences.\n",
        "        transformer_tokenizer (transformers.PreTrainedTokenizer):\n",
        "            tokenizer for transformer model.\n",
        "        transformer_config (transformers.PretrainedConfig): config\n",
        "            for transformer model.\n",
        "        max_len (int): Maximum length of sentence after applying\n",
        "            transformer tokenizer.\n",
        "        device (str): Computational device.\n",
        "        tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder\n",
        "            for Named-Entity tags.\n",
        "        tag_outside (str): Special 'outside' NER tag.\n",
        "        batch_size (int, optional): Batch Size for DataLoader.\n",
        "            Defaults to 8.\n",
        "        num_workers (int, optional): Number of workers. Defaults\n",
        "            to 1.\n",
        "        return_tensors (bool, optional): if True, return tensors.\n",
        "        return_confidence (bool, optional): if True, return\n",
        "            confidence scores for all predicted tokens. Defaults\n",
        "            to False.\n",
        "        pad_sequences (bool, optional): if True, pad sequences.\n",
        "            Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        List[List[str]]: List of lists with predicted Entity\n",
        "        tags.\n",
        "    \"\"\"\n",
        "    # make sure, that input has the correct format.\n",
        "    assert isinstance(sentences, list), \"'sentences' must be a list of list of word-tokens\"\n",
        "    assert isinstance(sentences[0], list), \"'sentences' must be a list of list of word-tokens\"\n",
        "    assert isinstance(sentences[0][0], str), \"'sentences' must be a list of list of word-tokens\"\n",
        "\n",
        "    # set network to appropriate mode.\n",
        "    network.eval()\n",
        "\n",
        "    # fill 'dummy' tags (expected input for dataloader).\n",
        "    tag_fill = [tag_encoder.classes_[0]]\n",
        "    tags_dummy = [tag_fill * len(sent) for sent in sentences]\n",
        "\n",
        "    dl = create_dataloader(sentences = sentences,\n",
        "                           tags = tags_dummy,\n",
        "                           transformer_tokenizer = transformer_tokenizer,\n",
        "                           transformer_config = transformer_config,\n",
        "                           max_len = max_len,\n",
        "                           batch_size = batch_size,\n",
        "                           tag_encoder = tag_encoder,\n",
        "                           tag_outside = tag_outside,\n",
        "                           num_workers = num_workers,\n",
        "                           pad_sequences = pad_sequences)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "    tensors = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, dl in enumerate(dl):\n",
        "\n",
        "            outputs = network(**dl)\n",
        "\n",
        "            # conduct operations on sentence level.\n",
        "            for i in range(outputs.shape[0]):\n",
        "\n",
        "                # extract prediction and transform.\n",
        "\n",
        "                # find max by row.\n",
        "                values, indices = outputs[i].max(dim=1)\n",
        "\n",
        "                preds = tag_encoder.inverse_transform(indices.cpu().numpy())\n",
        "                probs = values.cpu().numpy()\n",
        "\n",
        "                if return_tensors:\n",
        "                    tensors.append(outputs)\n",
        "\n",
        "                # subset predictions for original word tokens.\n",
        "                preds = [prediction for prediction, offset in zip(preds.tolist(), dl.get('offsets')[i]) if offset]\n",
        "                if return_confidence:\n",
        "                    probs = [prob for prob, offset in zip(probs.tolist(), dl.get('offsets')[i]) if offset]\n",
        "\n",
        "                # Remove special tokens ('CLS' + 'SEP').\n",
        "                preds = preds[1:-1]\n",
        "                if return_confidence:\n",
        "                    probs = probs[1:-1]\n",
        "\n",
        "                # make sure resulting predictions have same length as\n",
        "                # original sentence.\n",
        "\n",
        "                # TODO: Move assert statement to unit tests. Does not work\n",
        "                # in boundary.\n",
        "                # assert len(preds) == len(sentences[i])\n",
        "                predictions.append(preds)\n",
        "                if return_confidence:\n",
        "                    probabilities.append(probs)\n",
        "\n",
        "            if return_confidence:\n",
        "                return predictions, probabilities\n",
        "\n",
        "            if return_tensors:\n",
        "                return tensors\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def predict_text(network: torch.nn.Module,\n",
        "                 text: str,\n",
        "                 transformer_tokenizer: transformers.PreTrainedTokenizer,\n",
        "                 transformer_config: transformers.PretrainedConfig,\n",
        "                 max_len: int,\n",
        "                 device: str,\n",
        "                 tag_encoder: sklearn.preprocessing.LabelEncoder,\n",
        "                 tag_outside: str,\n",
        "                 batch_size: int = 8,\n",
        "                 num_workers: int = 1,\n",
        "                 pad_sequences: bool = True,\n",
        "                 return_confidence: bool = False,\n",
        "                 sent_tokenize: Callable = sent_tokenize,\n",
        "                 word_tokenize: Callable = word_tokenize) -> tuple:\n",
        "    \"\"\"Compute Predictions for Text.\n",
        "\n",
        "    Computes predictions for a text with `NERDA` model.\n",
        "    Text is tokenized into sentences before computing predictions.\n",
        "\n",
        "    Args:\n",
        "        network (torch.nn.Module): Network.\n",
        "        text (str): text to predict entities in.\n",
        "        transformer_tokenizer (transformers.PreTrainedTokenizer):\n",
        "            tokenizer for transformer model.\n",
        "        transformer_config (transformers.PretrainedConfig): config\n",
        "            for transformer model.\n",
        "        max_len (int): Maximum length of sentence after applying\n",
        "            transformer tokenizer.\n",
        "        device (str): Computational device.\n",
        "        tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder\n",
        "            for Named-Entity tags.\n",
        "        tag_outside (str): Special 'outside' NER tag.\n",
        "        batch_size (int, optional): Batch Size for DataLoader.\n",
        "            Defaults to 8.\n",
        "        num_workers (int, optional): Number of workers. Defaults\n",
        "            to 1.\n",
        "        pad_sequences (bool, optional): if True, pad sequences.\n",
        "            Defaults to True.\n",
        "        return_confidence (bool, optional): if True, return\n",
        "            confidence scores for predicted tokens. Defaults\n",
        "            to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: sentence- and word-tokenized text with corresponding\n",
        "        predicted named-entity tags.\n",
        "    \"\"\"\n",
        "    assert isinstance(text, str), \"'text' must be a string.\"\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    predictions = predict(network = network,\n",
        "                          sentences = sentences,\n",
        "                          transformer_tokenizer = transformer_tokenizer,\n",
        "                          transformer_config = transformer_config,\n",
        "                          max_len = max_len,\n",
        "                          device = device,\n",
        "                          return_confidence = return_confidence,\n",
        "                          batch_size = batch_size,\n",
        "                          num_workers = num_workers,\n",
        "                          pad_sequences = pad_sequences,\n",
        "                          tag_encoder = tag_encoder,\n",
        "                          tag_outside = tag_outside)\n",
        "\n",
        "    return sentences, predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4AmhEXJifK9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import warnings\n",
        "import transformers\n",
        "import sklearn.preprocessing\n",
        "\n",
        "class NERDADataSetReader():\n",
        "    \"\"\"Generic NERDA DataSetReader\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                sentences: list,\n",
        "                tags: list,\n",
        "                transformer_tokenizer: transformers.PreTrainedTokenizer,\n",
        "                transformer_config: transformers.PretrainedConfig,\n",
        "                max_len: int,\n",
        "                tag_encoder: sklearn.preprocessing.LabelEncoder,\n",
        "                tag_outside: str,\n",
        "                pad_sequences : bool = True) -> None:\n",
        "        \"\"\"Initialize DataSetReader\n",
        "\n",
        "        Initializes DataSetReader that prepares and preprocesses\n",
        "        DataSet for Named-Entity Recognition Task and training.\n",
        "\n",
        "        Args:\n",
        "            sentences (list): Sentences.\n",
        "            tags (list): Named-Entity tags.\n",
        "            transformer_tokenizer (transformers.PreTrainedTokenizer):\n",
        "                tokenizer for transformer.\n",
        "            transformer_config (transformers.PretrainedConfig): Config\n",
        "                for transformer model.\n",
        "            max_len (int): Maximum length of sentences after applying\n",
        "                transformer tokenizer.\n",
        "            tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder\n",
        "                for Named-Entity tags.\n",
        "            tag_outside (str): Special Outside tag.\n",
        "            pad_sequences (bool): Pad sequences to max_len. Defaults\n",
        "                to True.\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.tag_encoder = tag_encoder\n",
        "        self.pad_token_id = transformer_config.pad_token_id\n",
        "        self.tag_outside_transformed = tag_encoder.transform([tag_outside])[0]\n",
        "        self.pad_sequences = pad_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = self.sentences[item]\n",
        "        tags = self.tags[item]\n",
        "        # encode tags\n",
        "        tags = self.tag_encoder.transform(tags)\n",
        "\n",
        "        # check inputs for consistancy\n",
        "        assert len(sentence) == len(tags)\n",
        "\n",
        "        input_ids = []\n",
        "        target_tags = []\n",
        "        tokens = []\n",
        "        offsets = []\n",
        "\n",
        "        # for debugging purposes\n",
        "        # print(item)\n",
        "        for i, word in enumerate(sentence):\n",
        "            # bert tokenization\n",
        "            wordpieces = self.transformer_tokenizer.tokenize(word)\n",
        "            tokens.extend(wordpieces)\n",
        "            # make room for CLS if there is an identified word piece\n",
        "            if len(wordpieces)>0:\n",
        "                offsets.extend([1]+[0]*(len(wordpieces)-1))\n",
        "            # Extends the ner_tag if the word has been split by the wordpiece tokenizer\n",
        "            target_tags.extend([tags[i]] * len(wordpieces))\n",
        "\n",
        "        # Make room for adding special tokens (one for both 'CLS' and 'SEP' special tokens)\n",
        "        # max_len includes _all_ tokens.\n",
        "        if len(tokens) > self.max_len-2:\n",
        "            msg = f'Sentence #{item} length {len(tokens)} exceeds max_len {self.max_len} and has been truncated'\n",
        "            warnings.warn(msg)\n",
        "        tokens = tokens[:self.max_len-2]\n",
        "        target_tags = target_tags[:self.max_len-2]\n",
        "        offsets = offsets[:self.max_len-2]\n",
        "\n",
        "        # encode tokens for BERT\n",
        "        # TO DO: prettify this.\n",
        "        input_ids = self.transformer_tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = [self.transformer_tokenizer.cls_token_id] + input_ids + [self.transformer_tokenizer.sep_token_id]\n",
        "\n",
        "        # fill out other inputs for model.\n",
        "        target_tags = [self.tag_outside_transformed] + target_tags + [self.tag_outside_transformed]\n",
        "        masks = [1] * len(input_ids)\n",
        "        # set to 0, because we are not doing NSP or QA type task (across multiple sentences)\n",
        "        # token_type_ids distinguishes sentences.\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        offsets = [1] + offsets + [1]\n",
        "\n",
        "        # Padding to max length\n",
        "        # compute padding length\n",
        "        if self.pad_sequences:\n",
        "            padding_len = self.max_len - len(input_ids)\n",
        "            input_ids = input_ids + ([self.pad_token_id] * padding_len)\n",
        "            masks = masks + ([0] * padding_len)\n",
        "            offsets = offsets + ([0] * padding_len)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "            target_tags = target_tags + ([self.tag_outside_transformed] * padding_len)\n",
        "\n",
        "        return {'input_ids' : torch.tensor(input_ids, dtype = torch.long),\n",
        "                'masks' : torch.tensor(masks, dtype = torch.long),\n",
        "                'token_type_ids' : torch.tensor(token_type_ids, dtype = torch.long),\n",
        "                'target_tags' : torch.tensor(target_tags, dtype = torch.long),\n",
        "                'offsets': torch.tensor(offsets, dtype = torch.long)}\n",
        "\n",
        "def create_dataloader(sentences,\n",
        "                      tags,\n",
        "                      transformer_tokenizer,\n",
        "                      transformer_config,\n",
        "                      max_len,\n",
        "                      tag_encoder,\n",
        "                      tag_outside,\n",
        "                      batch_size = 1,\n",
        "                      num_workers = 1,\n",
        "                      pad_sequences = True):\n",
        "\n",
        "    if not pad_sequences and batch_size > 1:\n",
        "        print(\"setting pad_sequences to True, because batch_size is more than one.\")\n",
        "        pad_sequences = True\n",
        "\n",
        "    data_reader = NERDADataSetReader(\n",
        "        sentences = sentences,\n",
        "        tags = tags,\n",
        "        transformer_tokenizer = transformer_tokenizer,\n",
        "        transformer_config = transformer_config,\n",
        "        max_len = max_len,\n",
        "        tag_encoder = tag_encoder,\n",
        "        tag_outside = tag_outside,\n",
        "        pad_sequences = pad_sequences)\n",
        "        # Don't pad sequences if batch size == 1. This improves performance.\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        data_reader, batch_size = batch_size, num_workers = num_workers\n",
        "    )\n",
        "\n",
        "    return data_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnY-5dyhisEg"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def match_kwargs(function: Callable, **kwargs) -> dict:\n",
        "    \"\"\"Matches Arguments with Function\n",
        "\n",
        "    Match keywords arguments with the arguments of a function.\n",
        "\n",
        "    Args:\n",
        "        function (function): Function to match arguments for.\n",
        "        kwargs: keyword arguments to match against.\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary with matching arguments and their\n",
        "        respective values.\n",
        "\n",
        "    \"\"\"\n",
        "    arg_count = function.__code__.co_argcount\n",
        "    args = function.__code__.co_varnames[:arg_count]\n",
        "\n",
        "    args_dict = {}\n",
        "    for k, v in kwargs.items():\n",
        "        if k in args:\n",
        "            args_dict[k] = v\n",
        "\n",
        "    return args_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHyLsr1eif-z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import random\n",
        "import torch\n",
        "from IPython import get_ipython\n",
        "\n",
        "if True:\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            from tqdm.notebook import tqdm   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            from tqdm import tqdm # Terminal running IPython\n",
        "        else:\n",
        "            from tqdm import tqdm  # Other type (?)\n",
        "    except NameError:\n",
        "        from tqdm import tqdm\n",
        "\n",
        "def train(model, data_loader, optimizer, device, scheduler, n_tags):\n",
        "    \"\"\"One Iteration of Training\"\"\"\n",
        "\n",
        "    model.train()\n",
        "    final_loss = 0.0\n",
        "\n",
        "    for dl in tqdm(data_loader, total=len(data_loader)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**dl)\n",
        "        loss = compute_loss(outputs,\n",
        "                            dl.get('target_tags'),\n",
        "                            dl.get('masks'),\n",
        "                            device,\n",
        "                            n_tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "\n",
        "    # Return average loss\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "def validate(model, data_loader, device, n_tags):\n",
        "    \"\"\"One Iteration of Validation\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    final_loss = 0.0\n",
        "\n",
        "    for dl in tqdm(data_loader, total=len(data_loader)):\n",
        "\n",
        "        outputs = model(**dl)\n",
        "        loss = compute_loss(outputs,\n",
        "                            dl.get('target_tags'),\n",
        "                            dl.get('masks'),\n",
        "                            device,\n",
        "                            n_tags)\n",
        "        final_loss += loss.item()\n",
        "\n",
        "    # Return average loss.\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "def compute_loss(preds, target_tags, masks, device, n_tags):\n",
        "\n",
        "    # initialize loss function.\n",
        "    lfn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute active loss to not compute loss of paddings\n",
        "    active_loss = masks.view(-1) == 1\n",
        "\n",
        "    active_logits = preds.view(-1, n_tags)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target_tags.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target_tags)\n",
        "    )\n",
        "\n",
        "    active_labels = torch.as_tensor(active_labels, device = torch.device(device), dtype = torch.long)\n",
        "\n",
        "    # Only compute loss on actual token predictions\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def enforce_reproducibility(seed = 42) -> None:\n",
        "    \"\"\"Enforce Reproducibity\n",
        "\n",
        "    Enforces reproducibility of models to the furthest\n",
        "    possible extent. This is done by setting fixed seeds for\n",
        "    random number generation etcetera.\n",
        "\n",
        "    For atomic operations there is currently no simple way to\n",
        "    enforce determinism, as the order of parallel operations\n",
        "    is not known.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Fixed seed. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Sets seed manually for both CPU and CUDA\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # CUDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # System based\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def train_model(network,\n",
        "                tag_encoder,\n",
        "                tag_outside,\n",
        "                transformer_tokenizer,\n",
        "                transformer_config,\n",
        "                dataset_training,\n",
        "                dataset_validation,\n",
        "                max_len = 128,\n",
        "                train_batch_size = 16,\n",
        "                validation_batch_size = 8,\n",
        "                epochs = 5,\n",
        "                warmup_steps = 0,\n",
        "                learning_rate = 5e-5,\n",
        "                device = None,\n",
        "                fixed_seed = 42,\n",
        "                num_workers = 1):\n",
        "\n",
        "    if fixed_seed is not None:\n",
        "        enforce_reproducibility(fixed_seed)\n",
        "\n",
        "    # compute number of unique tags from encoder.\n",
        "    n_tags = tag_encoder.classes_.shape[0]\n",
        "\n",
        "    # prepare datasets for modelling by creating data readers and loaders\n",
        "    dl_train = create_dataloader(sentences = dataset_training.get('sentences'),\n",
        "                                 tags = dataset_training.get('tags'),\n",
        "                                 transformer_tokenizer = transformer_tokenizer,\n",
        "                                 transformer_config = transformer_config,\n",
        "                                 max_len = max_len,\n",
        "                                 batch_size = train_batch_size,\n",
        "                                 tag_encoder = tag_encoder,\n",
        "                                 tag_outside = tag_outside,\n",
        "                                 num_workers = num_workers)\n",
        "    dl_validate = create_dataloader(sentences = dataset_validation.get('sentences'),\n",
        "                                    tags = dataset_validation.get('tags'),\n",
        "                                    transformer_tokenizer = transformer_tokenizer,\n",
        "                                    transformer_config = transformer_config,\n",
        "                                    max_len = max_len,\n",
        "                                    batch_size = validation_batch_size,\n",
        "                                    tag_encoder = tag_encoder,\n",
        "                                    tag_outside = tag_outside,\n",
        "                                    num_workers = num_workers)\n",
        "\n",
        "    optimizer_parameters = network.parameters()\n",
        "\n",
        "    num_train_steps = int(len(dataset_training.get('sentences')) / train_batch_size * epochs)\n",
        "\n",
        "    optimizer = AdamW(optimizer_parameters, lr = learning_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps = warmup_steps, num_training_steps = num_train_steps\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    best_valid_loss = np.inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "        train_loss = train(network, dl_train, optimizer, device, scheduler, n_tags)\n",
        "        train_losses.append(train_loss)\n",
        "        valid_loss = validate(network, dl_validate, device, n_tags)\n",
        "\n",
        "        print(f\"Train Loss = {train_loss} Valid Loss = {valid_loss}\")\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_parameters = network.state_dict()\n",
        "            best_valid_loss = valid_loss\n",
        "\n",
        "    # return best model\n",
        "    network.load_state_dict(best_parameters)\n",
        "\n",
        "    return network, train_losses, best_valid_loss\n",
        "\n",
        "def on_task_update(task_id,fisher_dict,opt_param_dict, model, data_loader, optimizer, device, scheduler, n_tags, shared_model):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for dl in tqdm(data_loader, total=len(data_loader), desc='Computing Fisher Score and optimal parameters'):\n",
        "        outputs = model(**dl)\n",
        "        loss = compute_loss(outputs,\n",
        "                            dl.get('target_tags'),\n",
        "                            dl.get('masks'),\n",
        "                            device,\n",
        "                            n_tags)\n",
        "        loss.backward()\n",
        "\n",
        "    fisher_dict[task_id] = {}\n",
        "    opt_param_dict[task_id] = {}\n",
        "\n",
        "    for name,param in shared_model.named_parameters():\n",
        "        try:\n",
        "            opt_param_dict[task_id][name] = param.data.clone()\n",
        "            fisher_dict[task_id][name] = param.grad.data.clone().pow(2)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "def train_ewc(task_id,fisher_dict,opt_param_dict, model, data_loader, optimizer, device, scheduler, n_tags, shared_model, ewc_lambda):\n",
        "\n",
        "    model.train()\n",
        "    final_loss = 0.0\n",
        "\n",
        "    for dl in tqdm(data_loader, total=len(data_loader)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**dl)\n",
        "        loss = compute_loss(outputs,\n",
        "                            dl.get('target_tags'),\n",
        "                            dl.get('masks'),\n",
        "                            device,\n",
        "                            n_tags)\n",
        "        for task in range(task_id):\n",
        "            for name,param in shared_model.named_parameters():\n",
        "                try:\n",
        "                    fisher = fisher_dict[task][name]\n",
        "                    opt_param = opt_param_dict[task][name]\n",
        "                    loss += (fisher * (opt_param - param).pow(2)).sum() *ewc_lambda\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "\n",
        "    # Return average loss\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "def train_model_new_task(network,\n",
        "                tag_encoder,\n",
        "                tag_outside,\n",
        "                transformer_tokenizer,\n",
        "                transformer_config,\n",
        "                dataset_training,\n",
        "                dataset_validation,\n",
        "                max_len = 128,\n",
        "                train_batch_size = 16,\n",
        "                validation_batch_size = 8,\n",
        "                epochs = 5,\n",
        "                warmup_steps = 0,\n",
        "                learning_rate = 5e-5,\n",
        "                device = None,\n",
        "                fixed_seed = 42,\n",
        "                num_workers = 1,\n",
        "                task_id = 0,\n",
        "                fisher_dict = {},\n",
        "                opt_param_dict = {},\n",
        "                shared_model = None,\n",
        "                ewc_lambda = 0.2):\n",
        "\n",
        "    if fixed_seed is not None:\n",
        "        enforce_reproducibility(fixed_seed)\n",
        "\n",
        "    if shared_model is None:\n",
        "        shared_model = network\n",
        "\n",
        "    # compute number of unique tags from encoder.\n",
        "    n_tags = tag_encoder.classes_.shape[0]\n",
        "\n",
        "    # prepare datasets for modelling by creating data readers and loaders\n",
        "    dl_train = create_dataloader(sentences = dataset_training.get('sentences'),\n",
        "                                 tags = dataset_training.get('tags'),\n",
        "                                 transformer_tokenizer = transformer_tokenizer,\n",
        "                                 transformer_config = transformer_config,\n",
        "                                 max_len = max_len,\n",
        "                                 batch_size = train_batch_size,\n",
        "                                 tag_encoder = tag_encoder,\n",
        "                                 tag_outside = tag_outside,\n",
        "                                 num_workers = num_workers)\n",
        "    dl_validate = create_dataloader(sentences = dataset_validation.get('sentences'),\n",
        "                                    tags = dataset_validation.get('tags'),\n",
        "                                    transformer_tokenizer = transformer_tokenizer,\n",
        "                                    transformer_config = transformer_config,\n",
        "                                    max_len = max_len,\n",
        "                                    batch_size = validation_batch_size,\n",
        "                                    tag_encoder = tag_encoder,\n",
        "                                    tag_outside = tag_outside,\n",
        "                                    num_workers = num_workers)\n",
        "\n",
        "    optimizer_parameters = network.parameters()\n",
        "\n",
        "    num_train_steps = int(len(dataset_training.get('sentences')) / train_batch_size * epochs)\n",
        "\n",
        "    optimizer = AdamW(optimizer_parameters, lr = learning_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps = warmup_steps, num_training_steps = num_train_steps\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    best_valid_loss = np.inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "        train_loss = train_ewc(task_id,fisher_dict,opt_param_dict, network, dl_train, optimizer, device, scheduler, n_tags, shared_model, ewc_lambda)\n",
        "        train_losses.append(train_loss)\n",
        "        valid_loss = validate(network, dl_validate, device, n_tags)\n",
        "\n",
        "        print(f\"Train Loss = {train_loss} Valid Loss = {valid_loss}\")\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_parameters = network.state_dict()\n",
        "            best_valid_loss = valid_loss\n",
        "\n",
        "    on_task_update(task_id,fisher_dict,opt_param_dict, network, dl_train, optimizer, device, scheduler, n_tags, shared_model)\n",
        "    # return best model\n",
        "    # network.load_state_dict(best_parameters)\n",
        "\n",
        "    return network, train_losses, best_valid_loss\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2p8-x-Wilue",
        "outputId": "708b4ee3-bba9-4a10-a350-3b8c6ec43419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading https://data.deepai.org/conll2003.zip\n"
          ]
        }
      ],
      "source": [
        "from NERDA.datasets import get_conll_data, download_conll_data\n",
        "download_conll_data()\n",
        "training = get_conll_data('train')\n",
        "validation = get_conll_data('valid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VZVgM6_pnY2"
      },
      "outputs": [],
      "source": [
        "tag_scheme = [\n",
        "'B-PER',\n",
        "'I-PER',\n",
        "'B-ORG',\n",
        "'I-ORG',\n",
        "'B-LOC',\n",
        "'I-LOC',\n",
        "'B-MISC',\n",
        "'I-MISC'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUxPuW9upqSX"
      },
      "outputs": [],
      "source": [
        "transformer = 'bert-base-multilingual-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnwiBrS4psK7"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for network\n",
        "dropout = 0.1\n",
        "# hyperparameters for training\n",
        "training_hyperparameters = {\n",
        "'epochs' : 4,\n",
        "'warmup_steps' : 500,                                                   'train_batch_size': 13,                                         'learning_rate': 0.0001\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268,
          "referenced_widgets": [
            "a33fc51630504f4fa53aa8cbfe1f97f3",
            "9abb4ee3027941da919632dfdd0424c9",
            "11290b676b444f80b10a23203c5310eb",
            "3f538920afb14b62946d7f8599771865",
            "012183cbab5840e4bd30ccbf7b9e6b0c",
            "bf64840f5fc743f49be72e96a738a07a",
            "c737f6d66fb1465b874dcfcce14ea7b9",
            "d25dc4e66a944c13a00b0b87ebc38202",
            "618853d1c30146aa9ad3bbfd17434ffd",
            "2706c8e0b16a46c89d3754b9fe267723",
            "3e7176dd070a40e9a6e36d3f890cd091",
            "16d7bcc2e8924ff9987edd649063e85f",
            "2d15ac06faac4e7c8cf4ac68317a8d44",
            "1959707ea6944c95b65ecd971abcc08e",
            "e50aa8c37a2a45ba96408d699fe2d656",
            "77fa4b5f241b4cc2883b2a89e0e6fb2d",
            "d293b44275154cd096a6b2159aba2a9a",
            "626a1c3f666c49378f79a1e0950dc23c",
            "c7f2daaa7bca445facfdf16c5af4c713",
            "8917afad45054d63be94beec9e256a0c",
            "17b31d0c81ee4f97aa4fc17d285b9e0e",
            "bde03eaf49ca4426a15de5973c640a0d",
            "692c9235dac54628922769376fdc936b",
            "8e7f598382894199b0ec630d8e2d2e89",
            "33876120f82a47aebb6a534f3684aeb3",
            "ebe55f4cb6f0468c8a0eb51230b480bf",
            "6780cde7771f45b3ac74990f2059c3b3",
            "85d6a340800c4b7bbc7374121f35ecf1",
            "fbf794dfa04741d1885850af42b7c0a3",
            "d9b3e82ce00d4fd7880569a1da6f3819",
            "c6e31380da144fdcb3115d89b344bfeb",
            "ce4f88aef003432ba1b9e6341e352a35",
            "74e5472b6a9a4827908e773b12c118a5",
            "ce3dd40d8c054db0b7ecd6cd9a2e6f47",
            "a3ad573470d94c4581902462ab6d0435",
            "7779b2b7f9984c8186ed0a4f8b6cf55a",
            "3acbece62d4845c48ca0c4462af27058",
            "dc700fe43df2479f95febf9db4cb01ae",
            "4a4cc4fb4d1449acb6784818af49aecb",
            "f0f10d9002aa4ef7ab8130aa490b22bd",
            "994a40ac10004fe08a5cdf13a1a6e4a2",
            "091bb41da8a0433e80a9d101e4029c15",
            "828a0d9d29204ab79a7b7e816194cd42",
            "8117f26fa6504226bd67fde8848d5eeb",
            "e488ae7041df4890ac725b38c0cb3c30",
            "8f7b248d22384aceb7ab18cf8c0090c1",
            "0219424367e84b179dba85de8cf6c879",
            "1ecc989e8dda4d3fa77a0b8e99c7848d",
            "a7a233709041433ca7e7198ed4960925",
            "8879a7ade76e41d9896705dc28430159",
            "72eab60ddec64bd6b97df1d46d5c3e4e",
            "bf748e81c18846a6aec2f601d6880682",
            "c8b52950cc0140cc90f957f2516f89c2",
            "12830f6ece7c4db6b9272660d6c8a6fe",
            "b1fa40eb04cd465e94cdb40fb0639178"
          ]
        },
        "id": "GSlqi4Rjpu_4",
        "outputId": "3da0797c-695f-4206-d467-c18df3cbfdca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device automatically set to: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a33fc51630504f4fa53aa8cbfe1f97f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16d7bcc2e8924ff9987edd649063e85f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/672M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "692c9235dac54628922769376fdc936b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce3dd40d8c054db0b7ecd6cd9a2e6f47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/872k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e488ae7041df4890ac725b38c0cb3c30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from NERDA.models import NERDA\n",
        "model = NERDA(\n",
        "dataset_training = training,\n",
        "dataset_validation = validation,\n",
        "tag_scheme = tag_scheme,\n",
        "tag_outside = 'O',\n",
        "transformer = transformer,\n",
        "dropout = dropout,\n",
        "hyperparameters = training_hyperparameters\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BN54kIjpyIy",
        "outputId": "791823aa-663e-4256-ac3b-83ec14521563"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 950/1080 [4:29:53<37:18, 17.22s/it]"
          ]
        }
      ],
      "source": [
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAUzb0fZp078"
      },
      "outputs": [],
      "source": [
        "test = get_conll_data('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8EA7ZtFp3ul"
      },
      "outputs": [],
      "source": [
        "model.evaluate_performance(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDR9EJYFp6vi"
      },
      "outputs": [],
      "source": [
        "model.predict_text('Cristiano Ronaldo plays for Juventus FC')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11B1g4LKWcz02hgdO5jSiD-LKiqAmpbNp",
      "authorship_tag": "ABX9TyPI9y42QHT0skB530bGuL52",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012183cbab5840e4bd30ccbf7b9e6b0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0219424367e84b179dba85de8cf6c879": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf748e81c18846a6aec2f601d6880682",
            "max": 1715180,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8b52950cc0140cc90f957f2516f89c2",
            "value": 1715180
          }
        },
        "091bb41da8a0433e80a9d101e4029c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11290b676b444f80b10a23203c5310eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d25dc4e66a944c13a00b0b87ebc38202",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_618853d1c30146aa9ad3bbfd17434ffd",
            "value": 625
          }
        },
        "12830f6ece7c4db6b9272660d6c8a6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d7bcc2e8924ff9987edd649063e85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d15ac06faac4e7c8cf4ac68317a8d44",
              "IPY_MODEL_1959707ea6944c95b65ecd971abcc08e",
              "IPY_MODEL_e50aa8c37a2a45ba96408d699fe2d656"
            ],
            "layout": "IPY_MODEL_77fa4b5f241b4cc2883b2a89e0e6fb2d"
          }
        },
        "17b31d0c81ee4f97aa4fc17d285b9e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1959707ea6944c95b65ecd971abcc08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7f2daaa7bca445facfdf16c5af4c713",
            "max": 672271273,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8917afad45054d63be94beec9e256a0c",
            "value": 672271273
          }
        },
        "1ecc989e8dda4d3fa77a0b8e99c7848d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12830f6ece7c4db6b9272660d6c8a6fe",
            "placeholder": "​",
            "style": "IPY_MODEL_b1fa40eb04cd465e94cdb40fb0639178",
            "value": " 1.72M/1.72M [00:00&lt;00:00, 1.39MB/s]"
          }
        },
        "2706c8e0b16a46c89d3754b9fe267723": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d15ac06faac4e7c8cf4ac68317a8d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d293b44275154cd096a6b2159aba2a9a",
            "placeholder": "​",
            "style": "IPY_MODEL_626a1c3f666c49378f79a1e0950dc23c",
            "value": "Downloading: 100%"
          }
        },
        "33876120f82a47aebb6a534f3684aeb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9b3e82ce00d4fd7880569a1da6f3819",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6e31380da144fdcb3115d89b344bfeb",
            "value": 28
          }
        },
        "3acbece62d4845c48ca0c4462af27058": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_828a0d9d29204ab79a7b7e816194cd42",
            "placeholder": "​",
            "style": "IPY_MODEL_8117f26fa6504226bd67fde8848d5eeb",
            "value": " 872k/872k [00:00&lt;00:00, 1.95MB/s]"
          }
        },
        "3e7176dd070a40e9a6e36d3f890cd091": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f538920afb14b62946d7f8599771865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2706c8e0b16a46c89d3754b9fe267723",
            "placeholder": "​",
            "style": "IPY_MODEL_3e7176dd070a40e9a6e36d3f890cd091",
            "value": " 625/625 [00:00&lt;00:00, 5.69kB/s]"
          }
        },
        "4a4cc4fb4d1449acb6784818af49aecb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618853d1c30146aa9ad3bbfd17434ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "626a1c3f666c49378f79a1e0950dc23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6780cde7771f45b3ac74990f2059c3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692c9235dac54628922769376fdc936b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e7f598382894199b0ec630d8e2d2e89",
              "IPY_MODEL_33876120f82a47aebb6a534f3684aeb3",
              "IPY_MODEL_ebe55f4cb6f0468c8a0eb51230b480bf"
            ],
            "layout": "IPY_MODEL_6780cde7771f45b3ac74990f2059c3b3"
          }
        },
        "72eab60ddec64bd6b97df1d46d5c3e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74e5472b6a9a4827908e773b12c118a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7779b2b7f9984c8186ed0a4f8b6cf55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994a40ac10004fe08a5cdf13a1a6e4a2",
            "max": 871891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091bb41da8a0433e80a9d101e4029c15",
            "value": 871891
          }
        },
        "77fa4b5f241b4cc2883b2a89e0e6fb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8117f26fa6504226bd67fde8848d5eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "828a0d9d29204ab79a7b7e816194cd42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d6a340800c4b7bbc7374121f35ecf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8879a7ade76e41d9896705dc28430159": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8917afad45054d63be94beec9e256a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e7f598382894199b0ec630d8e2d2e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d6a340800c4b7bbc7374121f35ecf1",
            "placeholder": "​",
            "style": "IPY_MODEL_fbf794dfa04741d1885850af42b7c0a3",
            "value": "Downloading: 100%"
          }
        },
        "8f7b248d22384aceb7ab18cf8c0090c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8879a7ade76e41d9896705dc28430159",
            "placeholder": "​",
            "style": "IPY_MODEL_72eab60ddec64bd6b97df1d46d5c3e4e",
            "value": "Downloading: 100%"
          }
        },
        "994a40ac10004fe08a5cdf13a1a6e4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9abb4ee3027941da919632dfdd0424c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf64840f5fc743f49be72e96a738a07a",
            "placeholder": "​",
            "style": "IPY_MODEL_c737f6d66fb1465b874dcfcce14ea7b9",
            "value": "Downloading: 100%"
          }
        },
        "a33fc51630504f4fa53aa8cbfe1f97f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9abb4ee3027941da919632dfdd0424c9",
              "IPY_MODEL_11290b676b444f80b10a23203c5310eb",
              "IPY_MODEL_3f538920afb14b62946d7f8599771865"
            ],
            "layout": "IPY_MODEL_012183cbab5840e4bd30ccbf7b9e6b0c"
          }
        },
        "a3ad573470d94c4581902462ab6d0435": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a4cc4fb4d1449acb6784818af49aecb",
            "placeholder": "​",
            "style": "IPY_MODEL_f0f10d9002aa4ef7ab8130aa490b22bd",
            "value": "Downloading: 100%"
          }
        },
        "a7a233709041433ca7e7198ed4960925": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1fa40eb04cd465e94cdb40fb0639178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bde03eaf49ca4426a15de5973c640a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf64840f5fc743f49be72e96a738a07a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf748e81c18846a6aec2f601d6880682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e31380da144fdcb3115d89b344bfeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c737f6d66fb1465b874dcfcce14ea7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7f2daaa7bca445facfdf16c5af4c713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b52950cc0140cc90f957f2516f89c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce3dd40d8c054db0b7ecd6cd9a2e6f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3ad573470d94c4581902462ab6d0435",
              "IPY_MODEL_7779b2b7f9984c8186ed0a4f8b6cf55a",
              "IPY_MODEL_3acbece62d4845c48ca0c4462af27058"
            ],
            "layout": "IPY_MODEL_dc700fe43df2479f95febf9db4cb01ae"
          }
        },
        "ce4f88aef003432ba1b9e6341e352a35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d25dc4e66a944c13a00b0b87ebc38202": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d293b44275154cd096a6b2159aba2a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b3e82ce00d4fd7880569a1da6f3819": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc700fe43df2479f95febf9db4cb01ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e488ae7041df4890ac725b38c0cb3c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f7b248d22384aceb7ab18cf8c0090c1",
              "IPY_MODEL_0219424367e84b179dba85de8cf6c879",
              "IPY_MODEL_1ecc989e8dda4d3fa77a0b8e99c7848d"
            ],
            "layout": "IPY_MODEL_a7a233709041433ca7e7198ed4960925"
          }
        },
        "e50aa8c37a2a45ba96408d699fe2d656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b31d0c81ee4f97aa4fc17d285b9e0e",
            "placeholder": "​",
            "style": "IPY_MODEL_bde03eaf49ca4426a15de5973c640a0d",
            "value": " 672M/672M [00:16&lt;00:00, 48.2MB/s]"
          }
        },
        "ebe55f4cb6f0468c8a0eb51230b480bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce4f88aef003432ba1b9e6341e352a35",
            "placeholder": "​",
            "style": "IPY_MODEL_74e5472b6a9a4827908e773b12c118a5",
            "value": " 28.0/28.0 [00:00&lt;00:00, 757B/s]"
          }
        },
        "f0f10d9002aa4ef7ab8130aa490b22bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbf794dfa04741d1885850af42b7c0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}